{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f54a447-2c93-4f1f-be32-f9ddf1f937d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "Positive_sentiment_data =pd.read_csv('pos.csv',header=None,index_col=None)\n",
    "Negative_sentiment_data=pd.read_csv('neg.csv',header=None,index_col=None)\n",
    "Neutral_sentiment_data=pd.read_csv('neutral.csv',header=None,index_col=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d676a33f-4360-4c14-8e08-d90051052114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Henry Selick’s first movie since 2009’s Coraline. His fifth stop-motion masterpiece.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Positive_sentiment_data[0].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4f4f33-c0c4-4792-ba6a-7f815e81901f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Small pleasures aside, the movie doesn't offer anything particularly memorable or inventive.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Negative_sentiment_data[0].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d024855-5be5-4595-9cba-44589c860ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'कहते ैं ुाँ “सहस््ा्ी शहर” ैं – ाा ्ा ा सहस््ा्ी ीी ा े ा ीं ै ैॉ ें ैी ी ै ्् े ा् ो ीं ा ो ि् ाऊटर ा े ा ा ा ूँ ा ै ो ैॉ ा े ा ें े ूिा ीएचबीी ुाँ ा ि्ु् ो् ो ीं ा ा ै ाा ा ु ु ्ा ुाँ ैे े्ो ें ्े ि ाे े ि ्ा – ाँ ो ं े ाी िी ्ा े ि ू ें ो िा ा ा ाे ैं ूिों ो ्ा ाू ि िा े िी ोे ें ेे ाँ ें ा ा् े े्ो े ीं ्ाा िी ी ै '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Neutral_sentiment_data[0].iloc[4753]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62471dca-caad-4648-a454-79afd0af2cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41068719-cf6d-4ea5-a3a4-8567e5f3b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from contractions import fix\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# Ensure consistent language detection results\n",
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c397d96-dbca-4d05-8655-ea6fcd9b4042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b5dcd9b-85c7-4b0f-9593-568980484605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove digits from text\n",
    "def remove_digits(text):\n",
    "    text = str(text)\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Function to expand contractions (e.g., \"don't\" → \"do not\")\n",
    "def expand_contractions(text):\n",
    "    return fix(text)\n",
    "\n",
    "# Function to normalize elongated words (e.g., \"soooo\" → \"soo\")\n",
    "def normalize_elongated_words(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', text)  # replaces \"soooo\" with \"soo\"\n",
    "\n",
    "# Function to remove URLs, mentions (@user), and hashtags (#topic)\n",
    "def remove_urls_mentions_hashtags(text):\n",
    "    # Remove URLs (http:// or https:// links)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove mentions (@user) and hashtags (#topic)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    return text\n",
    "\n",
    "# Function to remove special characters and punctuation\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^\\w\\s]', \"\", text)  # Remove everything except alphanumeric and spaces\n",
    "\n",
    "# Function to clean HTML tags from text\n",
    "def remove_html_tags(text):\n",
    "    return re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "# Function to handle negations\n",
    "def handle_negations(doc):\n",
    "    tokens = []\n",
    "    skip_next = False\n",
    "    for i, token in enumerate(doc):\n",
    "        # Skip token if it was handled as a negation (already replaced)\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "        \n",
    "        # If the token is a negation word (like 'not'), append \"not\" and the next important word separately\n",
    "        if token.dep_ == 'neg' and i + 1 < len(doc):\n",
    "            next_token = doc[i + 1]\n",
    "            if not next_token.is_stop:\n",
    "                tokens.append('not')\n",
    "                tokens.append(next_token.lemma_)\n",
    "                skip_next = True  # Skip the next token because it's already handled\n",
    "        # Regular token processing\n",
    "        elif token.dep_ != 'neg' and not token.is_stop and token.pos_ not in ['PUNCT', 'PRON']:\n",
    "            tokens.append(token.lemma_)\n",
    "    return tokens\n",
    "\n",
    "# Function to retain important POS and handle negations\n",
    "def retain_important_pos(sentence):\n",
    "    doc = nlp(sentence.lower())\n",
    "    # Handle negations and keep adjectives, adverbs, verbs, and nouns\n",
    "    important_words = handle_negations(doc)\n",
    "    return ' '.join(important_words)\n",
    "\n",
    "# Function to remove non-English words\n",
    "def remove_non_english_words(text):\n",
    "    \"\"\"\n",
    "    Remove words that are not in English from the given text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "    \n",
    "    Returns:\n",
    "        str: The text containing only English words or those detected as English.\n",
    "    \"\"\"\n",
    "    words = text.split()  # Split text into words\n",
    "    retained_words = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            # Retain the word if detected as English\n",
    "            if detect(word) == \"en\" or word.isascii():\n",
    "                retained_words.append(word)\n",
    "        except LangDetectException:\n",
    "            retained_words.append(word)  # Retain the word if detection fails\n",
    "    return \" \".join(retained_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daf46074-cf02-49b3-8cf5-e5445a3bbcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and process the text using all steps\n",
    "def full_text_cleaning(text):\n",
    "    # Step 1: Remove digits\n",
    "    text = remove_digits(text)\n",
    "    \n",
    "    # Step 2: Remove URLs, mentions, and hashtags\n",
    "    text = remove_urls_mentions_hashtags(text)\n",
    "    \n",
    "    # Step 3: Remove HTML tags\n",
    "    text = remove_html_tags(text)\n",
    "    \n",
    "    # Step 4: Remove special characters and punctuation\n",
    "    text = remove_special_characters(text)\n",
    "    \n",
    "    # Step 5: Expand contractions\n",
    "    text = expand_contractions(text)\n",
    "    \n",
    "    # Step 6: Normalize elongated words\n",
    "    text = normalize_elongated_words(text)\n",
    "    \n",
    "    # Step 7: Lemmatize the text, retain important POS, and handle negations\n",
    "    text = retain_important_pos(text)\n",
    "    \n",
    "    # Step 8: Filter non-English text\n",
    "    text = remove_non_english_words(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "016c439b-d1b4-4973-8b82-cbb4c3f2ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the full_text_cleaning and split instead of word_tokenize\n",
    "Positive_sentiment_data[0] = Positive_sentiment_data[0].apply(lambda x: full_text_cleaning(x).split())\n",
    "Negative_sentiment_data[0] = Negative_sentiment_data[0].apply(lambda x: full_text_cleaning(x).split())\n",
    "Neutral_sentiment_data[0] = Neutral_sentiment_data[0].apply(lambda x: full_text_cleaning(x).split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a0cdec1-69f2-4250-97e6-cf69358aee08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['henry', 'selick', 'movie', 'coraline', 'fifth', 'stopmotion', 'masterpiece']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Positive_sentiment_data[0].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ac84048-d4c5-4490-8623-4678ba973a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cast',\n",
       " 'read',\n",
       " 'like',\n",
       " 'vogue',\n",
       " 'oscar',\n",
       " 'party',\n",
       " 'guest',\n",
       " 'list',\n",
       " 'valentine',\n",
       " 'day',\n",
       " 'cantmiss',\n",
       " 'cinema',\n",
       " 'instead',\n",
       " 'standard',\n",
       " 'hollywood',\n",
       " 'schmaltz']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Negative_sentiment_data[0].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12041771-929c-4836-baf1-afee15466bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Neutral_sentiment_data[0].iloc[4753]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65b1cac1-e18c-4bf4-83d1-ff71e08d0cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the library 'nltk'.if you don't have this library.then\n",
    "# use the command ' # !pip install nltk '\n",
    "#from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "932fa6d8-06cc-451d-99c1-34936d8d12cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all three sentiments into single array\n",
    "combined_data_set = np.concatenate((Positive_sentiment_data[0], Neutral_sentiment_data[0],Negative_sentiment_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1498a78-30fc-4d41-8ec1-b82201fb8a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['henry', 'selick', 'movie', 'coraline', 'fifth', 'stopmotion', 'masterpiece']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "799f25f9-c44b-49e3-8bb8-849c6b1a5183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28028"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Positive_sentiment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "188db816-42d9-43f4-8506-e2408b15775c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28926"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Negative_sentiment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9a62a70-00ee-4c4c-abe0-4a6ba2c66cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27673"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Neutral_sentiment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4366e730-abc3-4b51-98ba-c5c7fe2edd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84627"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b27bf714-b301-45fb-874d-be860e9a859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are converting positive sentiment to '1', negative sentiment to '-1', and neutral to '0'\n",
    "converted_data_set = np.concatenate((np.ones(len(Positive_sentiment_data), dtype=int),np.zeros(len(Neutral_sentiment_data),dtype=int),-1*np.ones(len(Negative_sentiment_data), dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8964f589-84f4-446a-af2e-bed5d945a45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84627"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f83d101a-d674-49d8-aa19-c7bc3a4f18c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84627"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(converted_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e55243f-8eb9-4c56-8dd6-4c456eb0f1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(combined_data_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "743987cd-0994-41b0-892d-fb0581959b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#array to list\n",
    "combined_data_set = combined_data_set.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14602235-e999-4a8a-b3f2-40ef17433ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(combined_data_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c50b721c-1ed9-470c-a8ff-2c88e6cf92a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_data_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5589fd1f-a153-4549-89ab-51d7a33f2fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_data_set[33333]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cf747bb-1105-4fde-af9c-39715ea0b5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_data_set[77777]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "219f5843-720c-4ff2-b162-bfcf6ef18125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWord2Vec: This is a popular model from the gensim library used for training word embeddings. \\n         It converts words into vectors of real numbers in a high-dimensional space, capturing their semantic meaning based on their context in a corpus. \\n         It's typically used in NLP tasks.\\n         \\nDictionary: This is a mapping between words and their integer IDs. \\n            It helps to create a vocabulary of unique words from a corpus, assigning each word a unique ID, \\n            which is useful for topic modeling and other NLP tasks where you need a compact representation of the text.\\n\\nsequence: This module from Keras provides utilities for processing sequences, like padding or truncating them to a fixed length. \\n         It's commonly used for preparing data to be fed into models, especially recurrent neural networks (RNNs),\\n        where input sequences need to have a uniform length.\\n\\nmultiprocessing: This module allows for parallel execution of code. \\n               It can be used to speed up tasks like training the Word2Vec model by leveraging multiple CPU cores.\\n\\n               \""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from keras.preprocessing import sequence\n",
    "import multiprocessing\n",
    "\n",
    "'''\n",
    "Word2Vec: This is a popular model from the gensim library used for training word embeddings. \n",
    "         It converts words into vectors of real numbers in a high-dimensional space, capturing their semantic meaning based on their context in a corpus. \n",
    "         It's typically used in NLP tasks.\n",
    "         \n",
    "Dictionary: This is a mapping between words and their integer IDs. \n",
    "            It helps to create a vocabulary of unique words from a corpus, assigning each word a unique ID, \n",
    "            which is useful for topic modeling and other NLP tasks where you need a compact representation of the text.\n",
    "\n",
    "sequence: This module from Keras provides utilities for processing sequences, like padding or truncating them to a fixed length. \n",
    "         It's commonly used for preparing data to be fed into models, especially recurrent neural networks (RNNs),\n",
    "        where input sequences need to have a uniform length.\n",
    "\n",
    "multiprocessing: This module allows for parallel execution of code. \n",
    "               It can be used to speed up tasks like training the Word2Vec model by leveraging multiple CPU cores.\n",
    "\n",
    "               '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9dc719f3-c1ce-4c7c-b215-7776a670bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_count = multiprocessing.cpu_count() #Returns the number of available CPU cores on your system. More cores allow for faster parallel processing, especially when training models.\n",
    "vocab_dim = 150 # The dimensionality of the word vectors (embeddings) generated by Word2Vec. A higher dimension can capture more information but increases memory and computation.\n",
    "n_iterations = 15 #The number of passes the Word2Vec algorithm makes over the training data. More iterations can lead to better embeddings but take more time.\n",
    "n_exposures = 15 #The minimum number of occurrences of a word for it to be included in the model's vocabulary. Words appearing fewer times are ignored.\n",
    "window_size = 7 # The number of words before and after a target word that Word2Vec will consider as its context. Larger windows capture broader context.\n",
    "n_epoch = 10 # The number of times the entire dataset is passed through the neural network during training. More epochs can improve the model but may risk overfitting.\n",
    "input_length = 100 # The length of input sequences for the model. Sentences longer than this will be truncated, and shorter ones will be padded.\n",
    "maxlen = 100 # The maximum allowable length for input sequences. Sentences longer than this value are truncated, while shorter ones are padded to this length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c9a6161-3384-407a-a20d-bf1bdf7f258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(model=None,combined_data_set=None):\n",
    "    if (combined_data_set is not None) and (model is not None):\n",
    "        gensim_dict = Dictionary()\n",
    "        gensim_dict.doc2bow(model.wv.index_to_key,allow_update=True)\n",
    "        \n",
    "        w2indx = {v: k+1 for k, v in gensim_dict.items()}\n",
    "        w2vec = {word: model.wv[word] for word in w2indx.keys()}\n",
    "\n",
    "        def parse_dataset(combined_data_set): # for transform words to integers\n",
    "            data=[]\n",
    "            for sentence in combined_data_set:\n",
    "                new_txt = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        new_txt.append(w2indx[word])\n",
    "                    except:\n",
    "                        new_txt.append(0) \n",
    "                data.append(new_txt)\n",
    "            return data \n",
    "            \n",
    "        combined_data_set = parse_dataset(combined_data_set)\n",
    "        combined_data_set = sequence.pad_sequences(combined_data_set, maxlen=maxlen)\n",
    "        return w2indx, w2vec,combined_data_set\n",
    "    else:\n",
    "        print( 'You are not provided any data')\n",
    "\n",
    "\n",
    "\n",
    "def word2vec_train(combined_data_set):\n",
    "\n",
    "    model = Word2Vec(vector_size=vocab_dim,\n",
    "                     min_count=n_exposures,\n",
    "                     window=window_size,\n",
    "                     workers=cpu_count,\n",
    "                     epochs=n_iterations)\n",
    "    model.build_vocab(combined_data_set) \n",
    "    model.train(combined_data_set, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    model.save('Word2vec_model.pkl')\n",
    "    index_dict, word_vectors,combined_data_set = create_dictionaries(model=model,combined_data_set=combined_data_set)\n",
    "    return   index_dict, word_vectors,combined_data_set\n",
    "\n",
    "#print ('Training a Word2vec model...')\n",
    "index_dict, word_vectors,combined_data_set = word2vec_train(combined_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "084366f7-662c-4c9c-a253-c920cdca99a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 2236, 5803, 3720],\n",
       "       [   0,    0,    0, ..., 4125, 2074,    0],\n",
       "       [   0,    0,    0, ..., 5535, 4379, 2867],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  242, 4484, 2040],\n",
       "       [   0,    0,    0, ...,    0, 4535,  326],\n",
       "       [   0,    0,    0, ..., 1022, 3583,    0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa17cf53-966b-41e2-9298-4882dbfdfe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Activation\n",
    "from keras.models import model_from_json\n",
    "np.random.seed(1337)  # For Reproducibility\n",
    "import sys\n",
    "sys.setrecursionlimit(1000000)\n",
    "import yaml\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "554232dd-c31a-4a08-b1d9-61d7a028c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2e94e64-3911-43f2-b500-28d7a8c435f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(index_dict,word_vectors,X,Y):\n",
    "\n",
    "    n_symbols = len(index_dict) + 1\n",
    "    embedding_weights = np.zeros((n_symbols, vocab_dim))\n",
    "    for word, index in index_dict.items():\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    y_train = keras.utils.to_categorical(y_train,num_classes=3) \n",
    "    y_test = keras.utils.to_categorical(y_test,num_classes=3)\n",
    "    # print x_train.shape,y_train.shape\n",
    "    return n_symbols,embedding_weights,x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d93c6edd-c01a-4ad8-b1b7-22eba4d81453",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_symbols,embedding_weights,x_train,y_train,x_test,y_test=get_data(index_dict, word_vectors,combined_data_set,converted_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "971be279-fb1c-4289-bc20-a6b28acce09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test):\n",
    "    # Defining a Simple Keras Model.\n",
    "    model = Sequential()  \n",
    "    model.add(Embedding(output_dim=vocab_dim,\n",
    "                        input_dim=n_symbols,\n",
    "                        mask_zero=True,\n",
    "                        weights=[embedding_weights],\n",
    "                        input_length=input_length))  # Adding Input Length\n",
    "    model.add(LSTM(units=50, activation='tanh', recurrent_activation='hard_sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax')) \n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=4,verbose=1)\n",
    "    # Predicting y_test using X_test\n",
    "    y_pred = model.predict(x_test, batch_size=batch_size)\n",
    "    \n",
    "    # Convert predicted probabilities to class labels (assuming categorical)\n",
    "    y_pred_classes = y_pred.argmax(axis=-1)\n",
    "    \n",
    "    # Convert one-hot encoded y_test to class labels\n",
    "    y_test_classes = y_test.argmax(axis=-1)\n",
    "    \n",
    "    # Calculate accuracy between predicted y_test and actual y_test\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "\n",
    "    #print( \"Evaluate...\")\n",
    "    score = model.evaluate(x_test, y_test,batch_size=batch_size)\n",
    "                                \n",
    "\n",
    "    json_string = model.to_json()\n",
    "    with open('lstm.json', 'w') as json_file:\n",
    "        json_file.write( json_string)\n",
    "    model.save_weights('lstm.weights.h5')\n",
    "    print ('Test score:')\n",
    "    print(score)\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f76c8eae-f93f-4841-a065-000a5ef0fc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2116/2116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 31ms/step - accuracy: 0.7271 - loss: 0.8325\n",
      "Epoch 2/4\n",
      "\u001b[1m2116/2116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - accuracy: 0.8487 - loss: 0.7063\n",
      "Epoch 3/4\n",
      "\u001b[1m2116/2116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 32ms/step - accuracy: 0.8792 - loss: 0.6738\n",
      "Epoch 4/4\n",
      "\u001b[1m2116/2116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 31ms/step - accuracy: 0.8991 - loss: 0.6533\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8574 - loss: 0.6917\n",
      "Test score:\n",
      "[0.6872574687004089, 0.8628146052360535]\n",
      "Accuracy: 86.28%\n"
     ]
    }
   ],
   "source": [
    "train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "42a76a3d-9b7b-42b3-9c8d-8b13c55d0bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_transform(string):\n",
    "    # Step 1: Clean the input string using the new cleaning pipeline\n",
    "    cleaned_text = full_text_cleaning(string)\n",
    "    \n",
    "    # Step 2: Tokenize the cleaned string (already lemmatized and cleaned)\n",
    "    words = cleaned_text.split()  # Tokenize based on space since SpaCy has cleaned it already\n",
    "    \n",
    "    # Step 3: Reshape the tokens for the model\n",
    "    words_array = np.array(words).reshape(1, -1)\n",
    "    # Load the pre-trained Word2Vec model\n",
    "    model = Word2Vec.load('Word2vec_model.pkl')\n",
    "    \n",
    "    # Step 4: Use the pre-trained Word2Vec model (or any model) for vector representation\n",
    "    _, _, combined_data_set = create_dictionaries(model, words_array)\n",
    "    \n",
    "    return combined_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "620b0dc1-3324-4ef0-9b64-f944309e0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "import streamlit as st\n",
    "\n",
    "def lstm_predict(string):\n",
    "    # Load the model architecture from JSON\n",
    "    with open('lstm.json', 'r') as json_file:\n",
    "        model_json = json_file.read()\n",
    "    model = model_from_json(model_json)\n",
    "\n",
    "    # Load the weights\n",
    "    model.load_weights('lstm.weights.h5')\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Preprocess the input\n",
    "    data = input_transform(string)\n",
    "    print(f\"Input shape before reshaping: {data.shape}\")\n",
    "    data = data.reshape(1, -1)  # Ensure correct input shape\n",
    "\n",
    "    # Get prediction from model\n",
    "    prediction = model.predict(data)\n",
    "    result = np.argmax(prediction, axis=-1)\n",
    "\n",
    "    # Map result to sentiment\n",
    "    if result[0] == 1:\n",
    "        print( \"Positive\")\n",
    "    elif result[0] == 0:\n",
    "        print( \"Neutral\")\n",
    "    else:\n",
    "        print( \"Negative\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d39d6bd0-4333-4844-9c60-659a5059ce8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape before reshaping: (1, 100)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step\n",
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "s = \" Hi, Brother \"\n",
    "lstm_predict(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d2200764-44d2-4fd0-abe1-c62d55b893bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape before reshaping: (1, 100)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "s = \" happy be happy \"\n",
    "lstm_predict(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "48d12001-f036-4001-be88-24eff47a081b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape before reshaping: (1, 100)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "s = \" bad to bad \"\n",
    "lstm_predict(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6c742-0567-409e-96d7-4d23cb0af98c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
